{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드\n",
    "\n",
    "https://www.kaggle.com/datasets/sitaberete/python-datascience-handbook-dataset-md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    from kaggle_secrets import UserSecretsClient\\n    import os\\n\\n    os.environ[\"LANGCHAIN_TRACING_V2\"] = \\'true\\'\\n    os.environ[\"LANGCHAIN_API_KEY\"] = UserSecrets\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    import os\n",
    "\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true'\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = UserSecrets\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:00<00:00, 12175.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    '/root/rag_test/data/python_data_science_handbook',\n",
    "    loader_cls = TextLoader,\n",
    "    glob = '*.md',\n",
    "    show_progress = True,\n",
    "    exclude = [\n",
    "        '05.15-Learning-More.md',\n",
    "        '06.00-Figure-Code.md'\n",
    "    ]\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 색인화(Indexing)\n",
    "\n",
    "주요 색인화 라이브러리\n",
    "\n",
    "- Chroma: LangChain에서 자주 사용되는 벡터 저장소 구현체입니다. 대규모 데이터셋에서 효율적인 유사성 검색을 수행할 수 있습니다.\n",
    "- Faiss: Facebook AI Research에서 개발한 라이브러리로, 고차원 벡터의 효율적인 유사성 검색과 클러스터링을 지원합니다.\n",
    "- Pinecone: 클라우드 기반의 벡터 데이터베이스로, 실시간 고차원 벡터 검색을 제공합니다.\n",
    "- Weaviate: 오픈소스 벡터 검색 엔진으로, GraphQL API를 통해 벡터 검색 기능을 제공합니다.\n",
    "- Qdrant: Rust로 작성된 벡터 데이터베이스로, 고성능 벡터 검색을 지원합니다.\n",
    "\n",
    "\n",
    "1. Chroma\n",
    "    - 오픈소스: ✅\n",
    "    - 주요 특징:\n",
    "        - LLM 애플리케이션 개발에 최적화\n",
    "        - 간단한 API로 사용 용이\n",
    "        - LangChain, LlamaIndex 등과 통합 지원\n",
    "    - 사용 사례: 로컬 개발 및 프로토타이핑에 적합\n",
    "\n",
    "2. Faiss (Facebook AI Similarity Search)\n",
    "    - 오픈소스: ✅\n",
    "    - 주요 특징:\n",
    "        - 대규모 유사성 검색에 매우 효율적\n",
    "        - GPU 지원으로 빠른 처리\n",
    "        - 다양한 인덱싱 방법 제공\n",
    "    - 사용 사례: 대규모 데이터셋에서의 고성능 유사성 검색\n",
    "\n",
    "3. Pinecone\n",
    "    - 오픈소스: ❌\n",
    "    - 주요 특징:\n",
    "        - 빠른 인덱싱 및 검색 성능\n",
    "        - 엔터프라이즈급 보안 (SOC 2, HIPAA 준수)\n",
    "        - 간편한 API와 클라우드 네이티브 통합\n",
    "    - 사용 사례: 엔터프라이즈급 보안이 필요한 프로덕션 환경\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Massive Text Embedding\n",
    "\n",
    "엄청나게 긴 글을 임베딩 하기에는 많은 문제가 있지만 무엇보다 모델의 토큰 수 제한이 가장 큰 걸림돌이다.\n",
    "\n",
    "BERT만 생각해도 512토큰이 한계이고 최근 큰 모델도 2048토큰 정도이다.\n",
    "\n",
    "하지만 이것보다 2048단어보다 더 긴 글을 임베딩 해야 할때는 어떻게 해야 할까?\n",
    "\n",
    "[https://huggingface.co/spaces/mteb/leaderboard]\n",
    "\n",
    "긴글을 임베딩 잘하는 모델들을 모아둔 leaderboard 이다. \n",
    "\n",
    "최근 모델에서는 NV-Embed-v2 모델이 1위 인데 우리가 사용하는 BAAI/bge-small-en 모델은 2위를 차지하고 있다.(해당 notebook이 만들어 졌을 때는 1위)\n",
    "\n",
    "어떻게 해결했는지 각 모델을 공부해야 알 것 같다.\n",
    "\n",
    "예전에 사용한 SBERT는 Sigment network(샴 네트워크) 구조를 사용해서 finetuning 함으로써 해결했었다.(? 사실상 검색에 더 적합한 임베딩을 만든 것이지 긴 글을 해결한 것은 아니다...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rag/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/rag/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/tmp/ipykernel_2018531/2702342486.py:12: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  child_docs_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap = 100)\n",
    "\n",
    "embedding_model = HuggingFaceBgeEmbeddings(model_name = 'BAAI/bge-small-en')\n",
    "\n",
    "child_docs_store = Chroma(\n",
    "    collection_name = \"split_parents\",\n",
    "    embedding_function = embedding_model\n",
    ")\n",
    "\n",
    "parent_docs_store = InMemoryStore()\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore = child_docs_store,\n",
    "    docstore = parent_docs_store,\n",
    "    child_splitter = child_splitter,\n",
    "    parent_splitter = parent_splitter,\n",
    "    search_kwargs = {\"k\" : 1},\n",
    ")\n",
    "\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검색 : Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html, clear_output\n",
    "\n",
    "def retrieve_docs(queries, retriever):\n",
    "    results = []\n",
    "\n",
    "    for query in queries:\n",
    "        retrieved_doc = retriever.get_relevant_documents(query)[0]\n",
    "        results.append(retrieved_doc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query : \n",
      " Missing Data\n",
      "\n",
      "relatant document :  # Handling Missing Data\n",
      "\n",
      "The difference between data found in many tutorials and data in the real world is that real-world data is rarely clean and homogeneous.\n",
      "In particular, many interesting datasets will have some amount of data missing.\n",
      "To make matters even more complicated, different data sources may indicate missing data in different ways.\n",
      "\n",
      "In this chapter, we will discuss some general considerations for missing data, look at how Pandas chooses to represent it, and explore some built-in Pandas tools for handling missing data in Python.\n",
      "Here and throughout the book, I will refer to missing data in general as *null*, *NaN*, or *NA* values.\n",
      "\n",
      "## Trade-offs in Missing Data Conventions\n",
      "\n",
      "A number of approaches have been developed to track the presence of missing data in a table or `DataFrame`.\n",
      "Generally, they revolve around one of two strategies: using a *mask* that globally indicates missing values, or choosing a *sentinel value* that indicates a missing entry.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query : \n",
      " What are missing data?\n",
      "\n",
      "relatant document :  # Handling Missing Data\n",
      "\n",
      "The difference between data found in many tutorials and data in the real world is that real-world data is rarely clean and homogeneous.\n",
      "In particular, many interesting datasets will have some amount of data missing.\n",
      "To make matters even more complicated, different data sources may indicate missing data in different ways.\n",
      "\n",
      "In this chapter, we will discuss some general considerations for missing data, look at how Pandas chooses to represent it, and explore some built-in Pandas tools for handling missing data in Python.\n",
      "Here and throughout the book, I will refer to missing data in general as *null*, *NaN*, or *NA* values.\n",
      "\n",
      "## Trade-offs in Missing Data Conventions\n",
      "\n",
      "A number of approaches have been developed to track the presence of missing data in a table or `DataFrame`.\n",
      "Generally, they revolve around one of two strategies: using a *mask* that globally indicates missing values, or choosing a *sentinel value* that indicates a missing entry.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query : \n",
      " Tell me what \"missing data\" means in data science\n",
      "\n",
      "relatant document :  # Handling Missing Data\n",
      "\n",
      "The difference between data found in many tutorials and data in the real world is that real-world data is rarely clean and homogeneous.\n",
      "In particular, many interesting datasets will have some amount of data missing.\n",
      "To make matters even more complicated, different data sources may indicate missing data in different ways.\n",
      "\n",
      "In this chapter, we will discuss some general considerations for missing data, look at how Pandas chooses to represent it, and explore some built-in Pandas tools for handling missing data in Python.\n",
      "Here and throughout the book, I will refer to missing data in general as *null*, *NaN*, or *NA* values.\n",
      "\n",
      "## Trade-offs in Missing Data Conventions\n",
      "\n",
      "A number of approaches have been developed to track the presence of missing data in a table or `DataFrame`.\n",
      "Generally, they revolve around one of two strategies: using a *mask* that globally indicates missing values, or choosing a *sentinel value* that indicates a missing entry.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query : \n",
      " Teach me something about missing data\n",
      "\n",
      "relatant document :  # Handling Missing Data\n",
      "\n",
      "The difference between data found in many tutorials and data in the real world is that real-world data is rarely clean and homogeneous.\n",
      "In particular, many interesting datasets will have some amount of data missing.\n",
      "To make matters even more complicated, different data sources may indicate missing data in different ways.\n",
      "\n",
      "In this chapter, we will discuss some general considerations for missing data, look at how Pandas chooses to represent it, and explore some built-in Pandas tools for handling missing data in Python.\n",
      "Here and throughout the book, I will refer to missing data in general as *null*, *NaN*, or *NA* values.\n",
      "\n",
      "## Trade-offs in Missing Data Conventions\n",
      "\n",
      "A number of approaches have been developed to track the presence of missing data in a table or `DataFrame`.\n",
      "Generally, they revolve around one of two strategies: using a *mask* that globally indicates missing values, or choosing a *sentinel value* that indicates a missing entry.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query : \n",
      " Naive Bayes\n",
      "\n",
      "relatant document :  # In Depth: Naive Bayes Classification\n",
      "\n",
      "The previous four chapters have given a general overview of the concepts of machine learning.\n",
      "In this chapter and the ones that follow, we will be taking a\n",
      "closer look first at four algorithms for  supervised learning,\n",
      "and then at four algorithms for unsupervised learning.\n",
      "We start here with our first supervised method, naive Bayes classification.\n",
      "\n",
      "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets.\n",
      "Because they are so fast and have so few tunable parameters, they end up being useful as a quick-and-dirty baseline for a classification problem.\n",
      "This chapter will provide an intuitive explanation of how naive Bayes classifiers work, followed by a few examples of them in action on some datasets.\n",
      "\n",
      "## Bayesian Classification\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query : \n",
      " What are the applications of naive bayes\n",
      "\n",
      "relatant document :  # In Depth: Naive Bayes Classification\n",
      "\n",
      "The previous four chapters have given a general overview of the concepts of machine learning.\n",
      "In this chapter and the ones that follow, we will be taking a\n",
      "closer look first at four algorithms for  supervised learning,\n",
      "and then at four algorithms for unsupervised learning.\n",
      "We start here with our first supervised method, naive Bayes classification.\n",
      "\n",
      "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets.\n",
      "Because they are so fast and have so few tunable parameters, they end up being useful as a quick-and-dirty baseline for a classification problem.\n",
      "This chapter will provide an intuitive explanation of how naive Bayes classifiers work, followed by a few examples of them in action on some datasets.\n",
      "\n",
      "## Bayesian Classification\n",
      "----------------------------------------------------------------------------------------------------\n",
      "query : \n",
      " Explain to me what naive bayes is\n",
      "\n",
      "relatant document :  # In Depth: Naive Bayes Classification\n",
      "\n",
      "The previous four chapters have given a general overview of the concepts of machine learning.\n",
      "In this chapter and the ones that follow, we will be taking a\n",
      "closer look first at four algorithms for  supervised learning,\n",
      "and then at four algorithms for unsupervised learning.\n",
      "We start here with our first supervised method, naive Bayes classification.\n",
      "\n",
      "Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets.\n",
      "Because they are so fast and have so few tunable parameters, they end up being useful as a quick-and-dirty baseline for a classification problem.\n",
      "This chapter will provide an intuitive explanation of how naive Bayes classifiers work, followed by a few examples of them in action on some datasets.\n",
      "\n",
      "## Bayesian Classification\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2018531/3753205412.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_doc = retriever.get_relevant_documents(query)[0]\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    'Missing Data',\n",
    "    'What are missing data?',\n",
    "    'Tell me what \"missing data\" means in data science',\n",
    "    'Teach me something about missing data',\n",
    "    'Naive Bayes',\n",
    "    'What are the applications of naive bayes',\n",
    "    'Explain to me what naive bayes is',\n",
    "]\n",
    "\n",
    "retrieved_results = retrieve_docs(queries, retriever)\n",
    "for result, query in zip(retrieved_results, queries):\n",
    " \n",
    "    content = result.page_content\n",
    "    print(\"query : \\n\", query)\n",
    "    print(\"\\nrelatant document : \", content)\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The Agent : Put it all together\n",
    "\n",
    "- 데이터(프롬프트)가 답하기 충분한 상태가 될때까지 검색한다?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "class CustomStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stops:tuple, tokenizer):\n",
    "        self.stops = stops\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, score: torch.FloatTensor, **kwargs) -> bool:\n",
    "        input = self.tokenizer.decode(input_ids[0])\n",
    "        return input.endswith(tuple(self.stops))\n",
    "\n",
    "class GemmaLangChain(LLM):\n",
    "    model : object\n",
    "    tokenizer : object\n",
    "    as_agent: bool = False\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return str(type(self.model))\n",
    "\n",
    "    def _call(self, prompt, stop, run_manager = None, **kwargs) -> str:\n",
    "        stopping_criteria = StoppingCriteriaList([\n",
    "            CustomStoppingCriteria(stop, self.tokenizer)\n",
    "        ]) if stop else None\n",
    "\n",
    "        input_ids = self.tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "        output = self.model.generate(**input_ids, max_new_tokens = 1024, stopping_criteria = stopping_criteria)\n",
    "\n",
    "        if self.as_agent:\n",
    "            output_text = self.tokenizer.decode(*output)[len(prompt) - 5:]\n",
    "        else:\n",
    "            output_text = self.tokenizer.decode(*output, skip_special_tokens = True)[len(prompt)]\n",
    "\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff3872ca3ab47b0ba35e4b5af37f8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model(model_id, tokenizer_id, load_in_4bit=True, device=\"auto\", quant_compute_dtype=torch.float16, torch_dtype=None):\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=quant_compute_dtype,\n",
    "    ) if load_in_4bit else None\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=device,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "    \n",
    "    return tokenizer, model\n",
    "    \n",
    "gemma_7b_it = \"google/gemma-7b-it\"\n",
    "gemma_7b_tokenizer, gemma_7b_model = load_model(gemma_7b_it, gemma_7b_it)\n",
    "gemma_langchain = GemmaLangChain(model = gemma_7b_model, tokenizer = gemma_7b_tokenizer, as_agent = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "stop_word = '\\nAction Output:'\n",
    "\n",
    "data_science_book = create_retriever_tool(\n",
    "    retriever,\n",
    "    'python_data_science_handbook',\n",
    "    'This tool teaches fundamental data science concepts. Use this tool for theore tical questions. Don\\'t rely on this tool for code implementation tasks. The input is a keyword(s).',\n",
    "    document_separator = \"\\n\\n\"\n",
    ")\n",
    "\n",
    "wikipedia = WikipediaQueryRun(\n",
    "    name = \"wikipedia\",\n",
    "    description = \"A wrapper around wikipedia that can provide up-to-date information on Data Science. The input must be a keyword(s)\",\n",
    "    api_wrapper = WikipediaAPIWrapper(\n",
    "        top_k_results = 1,\n",
    "        doc_content_chars_max = 1000\n",
    "    ),\n",
    ")\n",
    "\n",
    "@tool\n",
    "def programming_tool(question: str) -> str:\n",
    "    \"\"\"An LLM that writes codes that can perform a variety of Data Science Tasks. It implements computer code in Python, R, Tensorflow, Pytorch, Keras... Use this tool for programming/coding questions. The input must be a prompt.\"\"\"\n",
    "   \n",
    "    question = question.removesuffix(stop_word)\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant, the Coder from the DSI Crew (Data Science Instructor Crew). Your role is to write/implement computer code for a given Data Science task or question.\n",
    "\n",
    "    Make wure to produce a clear, detailed, expanatory and relevant code. Don't just write the code, explain it in detail and be professional with a teacher tone in your explanation.\n",
    "\n",
    "    Answer to the following question as best as you can according to the instructions above.\n",
    "    Question: {question}\n",
    "\n",
    "    Answer :\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = gemma_7b_tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "    output = gemma_7b_model.generate(**input_ids, max_new_tokens = 1024)\n",
    "    return gemma_7b_tokenizer.decode(*output, skip_special_tokens = True)[len(prompt):]\n",
    "\n",
    "programming_tool.description = programming_tool.description.removeprefix('programming_tool(prmpt: str) -> str - ')\n",
    "\n",
    "tools = [wikipedia, programming_tool, data_science_book]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "agent_prompt = PromptTemplate.from_template(\"\"\"Your role is to answer Data Science questions asked by a student.\n",
    "\n",
    "You have access to the following tools to help you answer the question:\n",
    "\n",
    "{tools}\n",
    "\n",
    "If you are asked questions about yourself or your crew, answer directly. Otherwise, use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: Always think about the next step. It could be thinking about the next tool to use or how to formulate the answer\n",
    "Action: the name of the tool to use, it has to be exactly one of [{tool_names}]. Only the name!\n",
    "Action Input: the input to the chosen tool\n",
    "Action Output: the output from the tool\n",
    "...(this Thought/Action/Action Input/Action Output can repeat N times)\n",
    "Thought: you now know the answer.\n",
    "Final Answer: your own detailed answer to the question.\n",
    "\n",
    "If a tool outputs something irrelevant to the question, ignore it and use another tool or change your input.\n",
    "Try to combine multiple tools to give the best answer. For instance, if the question is about data cleaning, use wikipedia or python_data_science_handbook to know what data cleaning is, then use the programming_tool to know how to implement it in code.\n",
    "Never use the same tool with the same input twice! \n",
    "Don't ask the user to use any tool! You are the only one who can use the tools.\n",
    "\n",
    "\n",
    "Begin!\n",
    "\n",
    "Answer the following questions as best as you can. Make sure to give a clear, detailed and relevant answer, and be professional with a teacher tone. \n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\")\n",
    "\n",
    "agent = create_react_agent(gemma_langchain, tools, agent_prompt, stop_sequence = [stop_word])\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent = agent,\n",
    "    tools = tools,\n",
    "    max_iterations = 5,\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought:\n",
      "Action: python_data_science_handbook\n",
      "Action Input: Feature Engineering\n",
      "Action Output:\u001b[0m\u001b[38;5;200m\u001b[1;3m# Feature Engineering\n",
      "\n",
      "The previous chapters outlined the fundamental ideas of machine learning, but all of the examples assumed that you have numerical data in a tidy, `[n_samples, n_features]` format.\n",
      "In the real world, data rarely comes in such a form.\n",
      "With this in mind, one of the more important steps in using machine learning in practice is *feature engineering*: that is, taking whatever information you have about your problem and turning it into numbers that you can use to build your feature matrix.\n",
      "\n",
      "In this chapter, we will cover a few common examples of feature engineering tasks: we'll look at features for representing categorical data, text, and images.\n",
      "Additionally, we will discuss derived features for increasing model complexity and imputation of missing data.\n",
      "This process is commonly referred to as vectorization, as it involves converting arbitrary data into well-behaved vectors.\n",
      "\n",
      "## Categorical Features\u001b[0m\u001b[32;1m\u001b[1;3mThought: \n",
      "Action: python_data_science_handbook\n",
      "Action Input: Categorical Features\n",
      "Action Output:\u001b[0m\u001b[38;5;200m\u001b[1;3m```python\n",
      "%matplotlib inline\n",
      "import seaborn as sns\n",
      "sns.pairplot(iris, hue='species', height=1.5);\n",
      "```\n",
      "\n",
      "\n",
      "    \n",
      "![png](05.02-Introducing-Scikit-Learn_files/05.02-Introducing-Scikit-Learn_9_0.png)\n",
      "    \n",
      "\n",
      "\n",
      "For use in Scikit-Learn, we will extract the features matrix and target array from the `DataFrame`, which we can do using some of the Pandas `DataFrame` operations discussed in [Part 3](03.00-Introduction-to-Pandas.ipynb):\n",
      "\n",
      "\n",
      "```python\n",
      "X_iris = iris.drop('species', axis=1)\n",
      "X_iris.shape\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    (150, 4)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "```python\n",
      "y_iris = iris['species']\n",
      "y_iris.shape\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    (150,)\n",
      "\n",
      "\n",
      "\n",
      "To summarize, the expected layout of features and target values is visualized in the following figure.\n",
      "\n",
      "![](images/05.02-samples-features.png)\n",
      "[figure source in Appendix](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/06.00-Figure-Code.ipynb#Features-and-Labels-Grid)\n",
      "\n",
      "With this data properly formatted, we can move on to consider Scikit-Learn's Estimator API.\n",
      "\n",
      "## The Estimator API\u001b[0m\u001b[32;1m\u001b[1;3mThought: \n",
      "Action: python_data_science_handbook\n",
      "Action Input: Estimator API\n",
      "Action Output:\u001b[0m\u001b[38;5;200m\u001b[1;3m### Basics of the API\n",
      "\n",
      "Most commonly, the steps in using the Scikit-Learn Estimator API are as follows:\n",
      "\n",
      "1. Choose a class of model by importing the appropriate estimator class from Scikit-Learn.\n",
      "2. Choose model hyperparameters by instantiating this class with desired values.\n",
      "3. Arrange data into a features matrix and target vector, as outlined earlier in this chapter.\n",
      "4. Fit the model to your data by calling the `fit` method of the model instance.\n",
      "5. Apply the model to new data:\n",
      "   - For supervised learning, often we predict labels for unknown data using the `predict` method.\n",
      "   - For unsupervised learning, we often transform or infer properties of the data using the `transform` or `predict` method.\n",
      "\n",
      "We will now step through several simple examples of applying supervised and unsupervised learning methods.\n",
      "\n",
      "### Supervised Learning Example: Simple Linear Regression\u001b[0m\u001b[32;1m\u001b[1;3mThought: \n",
      "Action: programming_tool\n",
      "Action Input: Simple Linear Regression\n",
      "Action Output:\u001b[0m\u001b[33;1m\u001b[1;3mImplement a simple linear regression model to predict the relationship between the number of bedrooms and the house price.\n",
      "\n",
      "**Additional Information:**\n",
      "\n",
      "- Use Python programming language.\n",
      "- Import necessary libraries.\n",
      "- Split the dataset into training and testing sets.\n",
      "- Train the linear regression model on the training set.\n",
      "- Evaluate the model performance on the testing set.\n",
      "\n",
      "**Expected Output:**\n",
      "\n",
      "- A clear and detailed Python code implementation.\n",
      "- Explanation of the code in detail.\n",
      "- Model performance evaluation on the testing set.\n",
      "\n",
      "**Please provide the code below:**\n",
      "\n",
      "```python\n",
      "# Import libraries\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('house_prices.csv')\n",
      "\n",
      "# Split the dataset into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(df['num_bedrooms'], df['price'], test_size=0.2, random_state=42)\n",
      "\n",
      "# Train the linear regression model\n",
      "model = LinearRegression()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "# Evaluate the model performance on the testing set\n",
      "score = model.score(X_test, y_test)\n",
      "print('The model score is:', score)\n",
      "```\n",
      "\n",
      "**Please explain the code in detail:**\n",
      "\n",
      "**1. Importing Libraries:**\n",
      "\n",
      "- The code imports necessary libraries like pandas, numpy, and scikit-learn.\n",
      "\n",
      "**2. Loading the Dataset:**\n",
      "\n",
      "- The dataset is loaded into a pandas DataFrame called 'df'.\n",
      "\n",
      "**3. Splitting the Dataset:**\n",
      "\n",
      "- The dataset is split into training and testing sets using the train_test_split function from scikit-learn.\n",
      "- The 'test_size' parameter is set to 0.2, indicating that 20% of the dataset will be used for testing.\n",
      "- The 'random_state' parameter is set to 42, which ensures reproducibility of the results.\n",
      "\n",
      "**4. Training the Linear Regression Model:**\n",
      "\n",
      "- A LinearRegression object is created.\n",
      "- The model is fit to the training data ('X_train', 'y_train') using the fit method.\n",
      "\n",
      "**5. Evaluating the Model Performance:**\n",
      "\n",
      "- The model performance is evaluated on the testing data ('X_test', 'y_test') using the score method.\n",
      "- The model score is printed to the console.\n",
      "\n",
      "**Additional Notes:**\n",
      "\n",
      "- The code assumes that the dataset file ('house_prices.csv') is available in the same directory as the script.\n",
      "- The code uses the scikit-learn library for linear regression modeling.\n",
      "- The model performance evaluation metrics can be further explored to assess the model's accuracy, precision, recall, etc.\u001b[0m\u001b[32;1m\u001b[1;3mThought: \n",
      "Final Answer: Feature Engineering is the process of transforming raw data into numerical features that are suitable for machine learning models. It involves various techniques such as extracting features from categorical data, text, images, and deriving new features from existing ones. The process is commonly referred to as vectorization and is essential for preparing data for machine learning model building.<eos>\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor.invoke({\"input\" : \"What is Feature Engineering?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought:\n",
      "I will use python_data_science_handbook to learn about house price predictors and then use programming_tool to write the code for implementation.\n",
      "\n",
      "Action: python_data_science_handbook(query='house price predictor')\n",
      "\n",
      "Action Input: N/A\n",
      "\n",
      "Action Output:\u001b[0mpython_data_science_handbook(query='house price predictor') is not a valid tool, try one of [wikipedia, programming_tool, python_data_science_handbook].\u001b[32;1m\u001b[1;3mThought: \n",
      "The tool is not valid, I will try again.\n",
      "\n",
      "Action: programming_tool(question='Write code to implement a house price predictor')\n",
      "\n",
      "Action Input: N/A\n",
      "\n",
      "Action Output:\u001b[0mprogramming_tool(question='Write code to implement a house price predictor') is not a valid tool, try one of [wikipedia, programming_tool, python_data_science_handbook].\u001b[32;1m\u001b[1;3mThought: \n",
      "The tool is not valid, I will try again.\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "I am unable to provide information on how to implement a house price predictor as I have not been able to successfully use the tools available to me. I recommend seeking additional resources or consulting with a data scientist to get more information on this topic.\n",
      "\n",
      "**Note:** This is just an example of how to answer the question. You can modify the format as needed.<eos>\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor.invoke({\"input\" : \"How to impletement a house price predictor\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma as A Routing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    '/root/rag_test/data/wikipedia_data_science_articles_summary',\n",
    "    loader_cls = TextLoader,\n",
    "    glob = '**/*.txt',\n",
    "    show_progress = True\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "embedding_model = HuggingFaceBgeEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
