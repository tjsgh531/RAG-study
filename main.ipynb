{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드\n",
    "\n",
    "https://www.kaggle.com/datasets/sitaberete/python-datascience-handbook-dataset-md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    from kaggle_secrets import UserSecretsClient\\n    import os\\n\\n    os.environ[\"LANGCHAIN_TRACING_V2\"] = \\'true\\'\\n    os.environ[\"LANGCHAIN_API_KEY\"] = UserSecrets\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    import os\n",
    "\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true'\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = UserSecrets\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:00<00:00, 19351.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    '/root/rag_test/data',\n",
    "    loader_cls = TextLoader,\n",
    "    glob = '*.md',\n",
    "    show_progress = True,\n",
    "    exclude = [\n",
    "        '05.15-Learning-More.md',\n",
    "        '06.00-Figure-Code.md'\n",
    "    ]\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 색인화(Indexing)\n",
    "\n",
    "- Faiss\n",
    "\n",
    "    1. 대규모 데이터셋을 다룰 때 (수백만 개 이상의 벡터)\n",
    "\n",
    "    2. 매우 빠른 검색 속도가 필요한 경우\n",
    "    \n",
    "    3. 저수준 제어가 필요한 고도로 최적화된 시스템을 구축할 때\n",
    "\n",
    "- Chroma\n",
    "\n",
    "    1. Chroma는 LangChain과의 통합이 매우 간단하고 직관적입니다.\n",
    "\n",
    "    2. Chroma는 단순한 벡터 검색을 넘어 문서 저장, 메타데이터 관리, 전체 텍스트 검색 등 더 많은 기능을 제공합니다.(RAG에 필요한 기능들 기본적으로 제공)\n",
    "\n",
    "    3. 소규모부터 중간 규모의 프로젝트에 적합하며, 클라우드 확장성도 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Massive Text Embedding\n",
    "\n",
    "엄청나게 긴 글을 임베딩 하기에는 많은 문제가 있지만 무엇보다 모델의 토큰 수 제한이 가장 큰 걸림돌이다.\n",
    "\n",
    "BERT만 생각해도 512토큰이 한계이고 최근 큰 모델도 2048토큰 정도이다.\n",
    "\n",
    "하지만 이것보다 2048단어보다 더 긴 글을 임베딩 해야 할때는 어떻게 해야 할까?\n",
    "\n",
    "[https://huggingface.co/spaces/mteb/leaderboard]\n",
    "\n",
    "긴글을 임베딩 잘하는 모델들을 모아둔 leaderboard 이다. \n",
    "\n",
    "최근 모델에서는 NV-Embed-v2 모델이 1위 인데 우리가 사용하는 BAAI/bge-small-en 모델은 2위를 차지하고 있다.(해당 notebook이 만들어 졌을 때는 1위)\n",
    "\n",
    "어떻게 해결했는지 각 모델을 공부해야 알 것 같다.\n",
    "\n",
    "예전에 사용한 SBERT는 Sigment network(샴 네트워크) 구조를 사용해서 finetuning 함으로써 해결했었다.(? 사실상 검색에 더 적합한 임베딩을 만든 것이지 긴 글을 해결한 것은 아니다...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPIEmbeddings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcccbf0b4444ac8bd74a076ef58acc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0400a01250e9451993064ce5c724bb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188e5390dd464d03ad2d90d5411245bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/90.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3bee9e697a4ec0b1055503a7bae368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db604bb7c8d948f48e735cc2d5cd9e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10e2f1f0e41417d8e0525b0dc686bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f637a5510a894a59b12d07bcbc88ccab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df932f50cf9a428f959705f0c825a726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c60eb058a0416ab2159130774ca722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65d8dc52e1b4f52bbbf4fec3aa8700d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a227024968644f88ea9e69a62233db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1892005/2702342486.py:12: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  child_docs_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap = 100)\n",
    "\n",
    "embedding_model = HuggingFaceBgeEmbeddings(model_name = 'BAAI/bge-small-en')\n",
    "\n",
    "child_docs_store = Chroma(\n",
    "    collection_name = \"split_parents\",\n",
    "    embedding_function = embedding_model\n",
    ")\n",
    "\n",
    "parent_docs_store = InMemoryStore()\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore = child_docs_store,\n",
    "    docstore = parent_docs_store,\n",
    "    child_splitter = child_splitter,\n",
    "    parent_splitter = parent_splitter,\n",
    "    search_kwargs = {\"k\" : 1},\n",
    ")\n",
    "\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
